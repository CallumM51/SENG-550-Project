{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c5ab32-cbbd-4007-a11a-a9b826a9ae8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read all csv files into a single RDD\n",
    "import os.path\n",
    "\n",
    "# reads all filenames that match the patter. Currently only two within the DBFS, 0819 and 0820\n",
    "files = '/FileStore/tables/*_UkraineCombinedTweetsDeduped.csv'\n",
    "tweets = spark.read.csv(files,header=True,sep=\",\",multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f66fdf-f3d6-48de-8aba-f52bff4340cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 'Dear vaccine advocate\\n\\nDo take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\\n\\nRegards\\n#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu'"
     ]
    }
   ],
   "source": [
    "# remove columns not needed\n",
    "dropped_tweets = tweets.drop(\"original_tweet_id\", \\\n",
    "                            \"original_tweet_userid\", \\\n",
    "                            \"original_tweet_username\", \\\n",
    "                            \"in_reply_to_status_id\", \\\n",
    "                            \"in_reply_to_user_id\", \\\n",
    "                            \"in_reply_to_screen_name\", \\\n",
    "                            \"quoted_status_id\", \\\n",
    "                            \"quoted_status_userid\", \\\n",
    "                            \"quoted_status_username\", \\\n",
    "                            \"extractedts\")\n",
    "\n",
    "# dropped_tweets.show(1)\n",
    "\n",
    "dropped_tweets.first()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beac797f-ca81-4595-9479-e78524833a0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only english tweets\n",
    "english_tweets = dropped_tweets.filter(dropped_tweets.language == \"en\" )\n",
    "# english_tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206a62ff-9baa-4c98-97d6-774cbbc35cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only non-retweets\n",
    "non_rt_tweets = english_tweets.filter(english_tweets.is_retweet == \"False\")\n",
    "# non_rt_tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e3870e-2165-495d-8301-ab524d797f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\nCollecting nltk>=3.1\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\nCollecting regex>=2021.8.3\n  Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.1)\nInstalling collected packages: tqdm, regex, nltk, textblob\nSuccessfully installed nltk-3.8.1 regex-2023.10.3 textblob-0.17.1 tqdm-4.66.1\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-1cb4807a-20ef-4330-bcc2-997a161378e2/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d4f265-6cec-4fe6-bc01-82e8d11b9de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-1cb4807a-20ef-4330-bcc2-997a161378e2/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52f56c5-b8b2-46c0-81d4-173d015275a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, least, lit, log\n",
    "\n",
    "def z_score_normalize(df, column, rangeVal = 100):\n",
    "    mean = df.agg({column: \"mean\"}).collect()[0][0]\n",
    "    stddev = df.agg({column: \"stddev\"}).collect()[0][0]\n",
    "    df = df.withColumn(column + \"_zscore\", (col(column) - mean) / stddev)\n",
    "    constant_column = lit(9)\n",
    "    df = df.withColumn(column + \"_normalized\", ((least(col(column + \"_zscore\"), constant_column) + 3) / 6)*rangeVal)\n",
    "    return df.drop(column + \"_zscore\", column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb0ab8-53d7-41b3-a4f9-8b23f81a3318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, to_date, least, lit, log\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# get average number of tweets per day from each user\n",
    "\n",
    "# make copy of dataframe and convert each tweet created time stamp into a time stamp object\n",
    "df = non_rt_tweets\n",
    "df = df.withColumn(\"date\", to_date(col(\"tweetcreatedts\")))\n",
    "\n",
    "# create window partitioned by user and ordered by date\n",
    "window = Window.partitionBy(\"username\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the number of tweets per day for each user\n",
    "df = df.withColumn(\"daily_tweet_count\", F.count(\"text\").over(window))\n",
    "\n",
    "# calculate average sentiment for each user\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_positive_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['pos']\n",
    "\n",
    "def get_negative_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['neg']\n",
    "\n",
    "# Register the sentiment analysis function as a Spark UDF\n",
    "sentiment_analysis_pos_udf = udf(get_positive_score, FloatType())\n",
    "sentiment_analysis_neg_udf = udf(get_negative_score, FloatType())\n",
    "\n",
    "# Apply sentiment analysis to each tweet and create a new column\n",
    "df = df.withColumn(\"positivity\", sentiment_analysis_pos_udf(col(\"text\")))\n",
    "df = df.withColumn(\"negativity\", sentiment_analysis_neg_udf(col(\"text\")))\n",
    "\n",
    "# Calculate the average sentimentality for each user and the average number of tweets per day\n",
    "avg_tweet_sentimentality_and_volume = df.groupBy(\"username\").agg(\n",
    "    # ((F.avg(\"sentiment_score\") + 1) * 50 * 2).alias(\"avg_sentimentality\"),\n",
    "    F.avg(\"daily_tweet_count\").alias(\"avg_tweets_per_day\"),\n",
    "    (F.avg(\"positivity\") * 300).alias(\"avg_positivity\"),\n",
    "    (F.avg(\"negativity\") * 300).alias(\"avg_negativity\")\n",
    ")\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.withColumn(\"avg_tweets_per_day\", log(1.5, (col(\"avg_tweets_per_day\") + 1)))\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = z_score_normalize(avg_tweet_sentimentality_and_volume, \"avg_tweets_per_day\", 70)\n",
    "\n",
    "# avg_tweet_sentimentality_and_volume.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40af10c-e252-4192-a9da-f981613aca9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize following and followers\n",
    "from pyspark.sql.functions import col, mean, count, sum, expr, least, lit, unix_timestamp, current_timestamp, log\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "current_time = current_timestamp()\n",
    "\n",
    "# Make sure you operate on unique users, in case of duplicates get the average for the user\n",
    "df_unique_users = english_tweets.groupBy(\"username\", \"usercreatedts\").agg(\n",
    "    sum((col(\"is_quote_status\") == \"True\").cast(\"int\")).alias(\"is_quote_status_true_count\"),\n",
    "    sum((col(\"is_quote_status\") == \"False\").cast(\"int\")).alias(\"is_quote_status_false_count\"),\n",
    "    mean(\"followers\").alias(\"followers\"),\n",
    "    mean(\"following\").alias(\"following\"),\n",
    "    count(\"username\").alias(\"tweet_count\"),\n",
    "    (unix_timestamp(current_time) - unix_timestamp(expr(\"substring(usercreatedts, 1, 19)\"))).cast(LongType()).alias(\"account_age\")\n",
    ")\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"followers\", log(10.0, col(\"followers\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"following\", log(7.0, col(\"following\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"tweet_count\", log(2.0, col(\"tweet_count\")))\n",
    "\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"followers\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"following\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"account_age\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"tweet_count\", 100)\n",
    "\n",
    "# Unique users\n",
    "df_unique_users = df_unique_users.withColumn(\"percentage_of_quotes\", 100*(col(\"is_quote_status_true_count\") / (col(\"is_quote_status_true_count\") + col(\"is_quote_status_false_count\"))))\n",
    "\n",
    "df_normalized_follow = df_unique_users.drop(\"is_quote_status_true_count\", \"is_quote_status_false_count\", \"usercreatedts\")\n",
    "\n",
    "# display(df_normalized_follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3e45cc-5dec-4a8b-95af-35b6afe8e513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "#Returns Hashtags within a string as an array\n",
    "def extractHastags(tweet):\n",
    "    return re.findall(r\"#(\\w+)\",tweet)\n",
    "\n",
    "\n",
    "#Resgister extractHastags as a Spark UDF\n",
    "extract_hashtags_udf = F.udf(extractHastags, ArrayType(StringType()))\n",
    "\n",
    "#Drop unused columns\n",
    "hashtag_df = english_tweets.drop(\"acctdesc\", \\\n",
    "                                \"location\", \\\n",
    "                                \"following\", \\\n",
    "                                \"followers\", \\\n",
    "                                \"totaltweets\", \\\n",
    "                                \"usercreatedts\", \\\n",
    "                                \"tweetcreatedts\", \\\n",
    "                                \"retweetcount\", \\\n",
    "                                \"language\", \\\n",
    "                                \"coordinates\", \\\n",
    "                                \"favorite_count\", \\\n",
    "                                \"is_retweet\", \\\n",
    "                                \"is_quote_status\", \\\n",
    "                                \"_c0\", \\\n",
    "                                \"userid\", \\\n",
    "                                \"tweetid\", \\\n",
    "                                )\n",
    "\n",
    "#Map Hashtags to users\n",
    "hashtag_df = hashtag_df.withColumn(\"hashtags\", extract_hashtags_udf(col(\"text\")))\n",
    "\n",
    "#Explode hastag list\n",
    "hashtag_df = hashtag_df.select(\"username\", F.explode(\"hashtags\").alias(\"hashtag\"))\n",
    "\n",
    "#Count the number of each hashtag by user\n",
    "grouped_hashtag_df = hashtag_df.groupBy(\"username\", \"hashtag\").count()\n",
    "\n",
    "#Calculate the max number of one hashtag for each user#\n",
    "result_hashtag_df = grouped_hashtag_df.groupBy(\"username\").agg(\n",
    "    log(2.0, F.max(\"count\")).alias(\"hashtags\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "hashtag_mean = result_hashtag_df.agg({\"hashtags\": \"mean\"}).collect()[0][0]\n",
    "hashtag_stddev = result_hashtag_df.agg({\"hashtags\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "constant_column = lit(9)\n",
    "\n",
    "result_hashtag_df = result_hashtag_df.withColumn(\"hashtags_zscore\", (col(\"hashtags\") - hashtag_mean) / hashtag_stddev)\n",
    "result_hashtag_df = result_hashtag_df.withColumn(\"hashtags_normalized\", ((least(col(\"hashtags_zscore\"), constant_column) + 3) / 6)*100)\n",
    "result_hashtag_df = result_hashtag_df.drop(\"hashtags_zscore\", \"hashtags\")\n",
    "#Display preview\n",
    "# result_hashtag_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5adc535f-279c-473f-8a85-014353cfd6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "#Drop unused columns\n",
    "count_df = english_tweets.drop(\"acctdesc\", \\\n",
    "                                \"location\", \\\n",
    "                                \"following\", \\\n",
    "                                \"followers\", \\\n",
    "                                \"text\", \\\n",
    "                                \"usercreatedts\", \\\n",
    "                                \"tweetcreatedts\", \\\n",
    "                                \"retweetcount\", \\\n",
    "                                \"language\", \\\n",
    "                                \"coordinates\", \\\n",
    "                                \"favorite_count\", \\\n",
    "                                \"is_retweet\", \\\n",
    "                                \"is_quote_status\", \\\n",
    "                                \"_c0\", \\\n",
    "                                \"userid\", \\\n",
    "                                \"tweetid\", \\\n",
    "                                \"hashtags\"\n",
    "                                )\n",
    "\n",
    "def calc_ratio(arr):\n",
    "    count = arr[0]\n",
    "    diff = arr[1]\n",
    "    if diff:\n",
    "        if count > diff:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return (count/diff)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "calc_ratio_udf = F.udf(calc_ratio, FloatType())\n",
    "\n",
    "count_df1 = count_df.groupBy(\"username\").count()\n",
    "\n",
    "#Calculate the max number of one hashtag for each user\n",
    "count_df2 = count_df.groupBy(\"username\").agg(\n",
    "    F.max(\"totaltweets\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "count_df3 = count_df.groupBy(\"username\").agg(\n",
    "    F.min(\"totaltweets\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "proportion_df = count_df1.join(count_df2,on=\"username\",how=\"left\")\n",
    "\n",
    "range_df = proportion_df.join(count_df3,on=\"username\",how=\"left\")\n",
    "\n",
    "\n",
    "range_df = range_df.withColumn(\"diff\", col(\"max(totaltweets)\")- col(\"min(totaltweets)\"))\n",
    "range_df = range_df.withColumn(\"ratio\", calc_ratio_udf(F.array( col(\"count\"),col(\"diff\") ))*100)\n",
    "range_df = range_df.drop(\"max(totaltweets)\", \"min(totaltweets)\", \"diff\", \"count\")\n",
    "\n",
    "# range_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba53e30-52b4-4ef0-b25d-f323bfd87f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_non_rt = avg_tweet_volume.join(avg_tweet_sentimentality, on=\"username\", how=\"inner\")\n",
    "# df_final = df_final_non_rt.join(df_normalized_follow, on=\"username\", how=\"left\").join(df_quote_percentage, on=\"username\", how=\"inner\").join(df_account_age, on=\"username\", how=\"inner\").join(username_counts, on=\"username\", how=\"inner\")\n",
    "df_final = avg_tweet_sentimentality_and_volume.join(df_normalized_follow, on=\"username\", how=\"left\").join(result_hashtag_df, on=\"username\", how=\"inner\").join(range_df, on=\"username\", how=\"inner\")\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0056683-3a06-4569-bc9b-f34ba994de87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "selected_columns = [col for col in df_final.columns if col not in ['username', 'userid']]\n",
    "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_final)\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 8\n",
    "\n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans().setK(k).setSeed(1)\n",
    "\n",
    "# Fit the model to the assembled DataFrame\n",
    "model = kmeans.fit(df_assembled)\n",
    "\n",
    "# Get the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "predictions = model.transform(df_assembled)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cluster Centers:\")\n",
    "for index, center in enumerate(centers):\n",
    "    print(index, \" \", center)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "# predictions.select(\"username\", \"features\", \"prediction\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff2268d-40bb-4a23-8343-54285c4c9fd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4127128177085360>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mpredictions\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musername\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4127128177085360>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpredictions\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musername\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\n\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'predictions' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions.select(\"username\", \"features\", \"prediction\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4127128177085355,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SENG 550 Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
