{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c5ab32-cbbd-4007-a11a-a9b826a9ae8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read all csv files into a single RDD\n",
    "import os.path\n",
    "\n",
    "# reads all filenames that match the patter. Currently only two within the DBFS, 0819 and 0820\n",
    "files = '/FileStore/tables/*_UkraineCombinedTweetsDeduped.csv'\n",
    "tweets = spark.read.csv(files,header=True,sep=\",\",multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f66fdf-f3d6-48de-8aba-f52bff4340cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: 'Dear vaccine advocate\\n\\nDo take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\\n\\nRegards\\n#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu'"
     ]
    }
   ],
   "source": [
    "# remove columns not needed\n",
    "dropped_tweets = tweets.drop(\"original_tweet_id\", \\\n",
    "                            \"original_tweet_userid\", \\\n",
    "                            \"original_tweet_username\", \\\n",
    "                            \"in_reply_to_status_id\", \\\n",
    "                            \"in_reply_to_user_id\", \\\n",
    "                            \"in_reply_to_screen_name\", \\\n",
    "                            \"quoted_status_id\", \\\n",
    "                            \"quoted_status_userid\", \\\n",
    "                            \"quoted_status_username\", \\\n",
    "                            \"extractedts\")\n",
    "\n",
    "# dropped_tweets.show(1)\n",
    "\n",
    "dropped_tweets.first()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beac797f-ca81-4595-9479-e78524833a0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only english tweets\n",
    "english_tweets = dropped_tweets.filter(dropped_tweets.language == \"en\" )\n",
    "# english_tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206a62ff-9baa-4c98-97d6-774cbbc35cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only non-retweets\n",
    "non_rt_tweets = english_tweets.filter(english_tweets.is_retweet == \"False\")\n",
    "# non_rt_tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e3870e-2165-495d-8301-ab524d797f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/lib/python3.9/site-packages (0.17.1)\nRequirement already satisfied: nltk>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/lib/python3.9/site-packages (from textblob) (3.8.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.66.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2023.10.3)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.1)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbab8b3-31e8-4b8c-afa9-ccdfc7f0df03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/lib/python3.9/site-packages (3.3.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ca6db4e-da87-4859-819c-1a4b7b669c4b/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e1b429-ba4f-4ed3-bc8a-ecc9f0a7ea4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, to_date, least, lit, log\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# get average number of tweets per day from each user\n",
    "\n",
    "# make copy of dataframe and convert each tweet created time stamp into a time stamp object\n",
    "df = non_rt_tweets\n",
    "df = df.withColumn(\"date\", to_date(col(\"tweetcreatedts\")))\n",
    "\n",
    "# create window partitioned by user and ordered by date\n",
    "window = Window.partitionBy(\"username\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the number of tweets per day for each user\n",
    "df = df.withColumn(\"daily_tweet_count\", F.count(\"text\").over(window))\n",
    "\n",
    "# calculate average sentiment for each user\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_positive_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['pos']\n",
    "\n",
    "def get_negative_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['neg']\n",
    "\n",
    "# Register the sentiment analysis function as a Spark UDF\n",
    "sentiment_analysis_pos_udf = udf(get_positive_score, FloatType())\n",
    "sentiment_analysis_neg_udf = udf(get_negative_score, FloatType())\n",
    "\n",
    "# Apply sentiment analysis to each tweet and create a new column\n",
    "df = df.withColumn(\"positivity\", sentiment_analysis_pos_udf(col(\"text\")))\n",
    "df = df.withColumn(\"negativity\", sentiment_analysis_neg_udf(col(\"text\")))\n",
    "\n",
    "# Calculate the average sentimentality for each user and the average number of tweets per day\n",
    "avg_tweet_sentimentality_and_volume = df.groupBy(\"username\").agg(\n",
    "    # ((F.avg(\"sentiment_score\") + 1) * 50 * 2).alias(\"avg_sentimentality\"),\n",
    "    F.avg(\"daily_tweet_count\").alias(\"avg_tweets_per_day\"),\n",
    "    (F.avg(\"positivity\") * 300).alias(\"avg_positivity\"),\n",
    "    (F.avg(\"negativity\") * 300).alias(\"avg_negativity\")\n",
    ")\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.withColumn(\"avg_tweets_per_day\", log(1.5, (col(\"avg_tweets_per_day\") + 1)))\n",
    "\n",
    "# Calculate the mean and stddev foravg_tweets_per_day\n",
    "avg_tweets_mean = avg_tweet_sentimentality_and_volume.agg({\"avg_tweets_per_day\": \"mean\"}).collect()[0][0]\n",
    "avg_tweets_stddev = avg_tweet_sentimentality_and_volume.agg({\"avg_tweets_per_day\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "constant_column = lit(9)\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.withColumn(\"avg_tweets_per_day_zscore\", (col(\"avg_tweets_per_day\") - avg_tweets_mean) / avg_tweets_stddev)\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.withColumn(\"avg_tweets_per_day_normalized\", ((least(col(\"avg_tweets_per_day_zscore\"), constant_column) + 3) / 6)*70)\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.drop(\"avg_tweets_per_day_zscore\", \"avg_tweets_per_day\")\n",
    "\n",
    "# avg_tweet_sentimentality_and_volume.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8999ec-8cc0-4208-8b07-985fe8469355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize following and followers\n",
    "from pyspark.sql.functions import col, mean, count, sum, expr, least, lit, unix_timestamp, current_timestamp, log\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "current_time = current_timestamp()\n",
    "\n",
    "# Make sure you operate on unique users, in case of duplicates get the average for the user\n",
    "df_unique_users = english_tweets.groupBy(\"username\", \"usercreatedts\").agg(\n",
    "    sum((col(\"is_quote_status\") == \"True\").cast(\"int\")).alias(\"is_quote_status_true_count\"),\n",
    "    sum((col(\"is_quote_status\") == \"False\").cast(\"int\")).alias(\"is_quote_status_false_count\"),\n",
    "    mean(\"followers\").alias(\"followers\"),\n",
    "    mean(\"following\").alias(\"following\"),\n",
    "    count(\"username\").alias(\"tweet_count\"),\n",
    "    (unix_timestamp(current_time) - unix_timestamp(expr(\"substring(usercreatedts, 1, 19)\"))).cast(LongType()).alias(\"account_age\")\n",
    ")\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"followers\", log(10.0, col(\"followers\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"following\", log(7.0, col(\"following\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"tweet_count\", log(2.0, col(\"tweet_count\")))\n",
    "\n",
    "# Calculate the mean and stddev for followers and following\n",
    "followers_mean = df_unique_users.agg({\"followers\": \"mean\"}).collect()[0][0]\n",
    "followers_stddev = df_unique_users.agg({\"followers\": \"stddev\"}).collect()[0][0]\n",
    "following_mean = df_unique_users.agg({\"following\": \"mean\"}).collect()[0][0]\n",
    "following_stddev = df_unique_users.agg({\"following\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "# Calculate the mean and stddev for account age\n",
    "account_age_mean = df_unique_users.agg({\"account_age\": \"mean\"}).collect()[0][0]\n",
    "account_age_stddev = df_unique_users.agg({\"account_age\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "# Calculate the mean and stddev for tweet count\n",
    "tweet_count_mean = df_unique_users.agg({\"tweet_count\": \"mean\"}).collect()[0][0]\n",
    "tweet_count_stddev = df_unique_users.agg({\"tweet_count\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "# Calculate the z_scores for each entry\n",
    "df_unique_users = df_unique_users.withColumn(\"followers_zscore\", (col(\"followers\") - followers_mean) / followers_stddev)\n",
    "df_unique_users = df_unique_users.withColumn(\"following_zscore\", (col(\"following\") - following_mean) / following_stddev)\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"account_age_zscore\", (col(\"account_age\") - account_age_mean) / account_age_stddev)\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"tweet_count_zscore\", (col(\"tweet_count\") - tweet_count_mean) / tweet_count_stddev)\n",
    "\n",
    "# Use lit to create a constant column with the integer value\n",
    "# (constant_column + 3) / 6 *100\n",
    "constant_column = lit(9)\n",
    "\n",
    "# Calculate normalized values\n",
    "# 99.7% of the data lie within 3 standard deviations of the mean, these values will be between 0-100, outliers will be outside this range\n",
    "df_unique_users = df_unique_users.withColumn(\"followers_normalized\", ((least(col(\"followers_zscore\"), constant_column) + 3) / 6)*100)\n",
    "df_unique_users = df_unique_users.withColumn(\"following_normalized\", ((least(col(\"following_zscore\"), constant_column) + 3) / 6)*100)\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"account_age_normalized\", ((least(col(\"account_age_zscore\"), constant_column) + 3) / 6)*100)\n",
    "df_unique_users = df_unique_users.withColumn(\"tweet_count_normalized\", ((least(col(\"tweet_count_zscore\"), constant_column) + 3) / 6)*100)\n",
    "\n",
    "# Unique users\n",
    "df_unique_users = df_unique_users.withColumn(\"percentage_of_quotes\", 100*(col(\"is_quote_status_true_count\") / (col(\"is_quote_status_true_count\") + col(\"is_quote_status_false_count\"))))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_normalized_follow = df_unique_users.drop(\"followers_zscore\", \"following_zscore\", \"followers\", \"following\", \"is_quote_status_true_count\", \"is_quote_status_false_count\", \"usercreatedts\", \"account_age_zscore\", \"account_age\", \"tweet_count\", \"tweet_count_zscore\")\n",
    "\n",
    "# display(df_normalized_follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3e45cc-5dec-4a8b-95af-35b6afe8e513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "#Returns Hashtags within a string as an array\n",
    "def extractHastags(tweet):\n",
    "    return re.findall(r\"#(\\w+)\",tweet)\n",
    "\n",
    "\n",
    "#Resgister extractHastags as a Spark UDF\n",
    "extract_hashtags_udf = F.udf(extractHastags, ArrayType(StringType()))\n",
    "\n",
    "#Drop unused columns\n",
    "hashtag_df = english_tweets.drop(\"acctdesc\", \\\n",
    "                                \"location\", \\\n",
    "                                \"following\", \\\n",
    "                                \"followers\", \\\n",
    "                                \"totaltweets\", \\\n",
    "                                \"usercreatedts\", \\\n",
    "                                \"tweetcreatedts\", \\\n",
    "                                \"retweetcount\", \\\n",
    "                                \"language\", \\\n",
    "                                \"coordinates\", \\\n",
    "                                \"favorite_count\", \\\n",
    "                                \"is_retweet\", \\\n",
    "                                \"is_quote_status\", \\\n",
    "                                \"_c0\", \\\n",
    "                                \"userid\", \\\n",
    "                                \"tweetid\", \\\n",
    "                                )\n",
    "\n",
    "#Map Hashtags to users\n",
    "hashtag_df = hashtag_df.withColumn(\"hashtags\", extract_hashtags_udf(col(\"text\")))\n",
    "\n",
    "#Explode hastag list\n",
    "hashtag_df = hashtag_df.select(\"username\", F.explode(\"hashtags\").alias(\"hashtag\"))\n",
    "\n",
    "#Count the number of each hashtag by user\n",
    "grouped_hashtag_df = hashtag_df.groupBy(\"username\", \"hashtag\").count()\n",
    "\n",
    "#Calculate the max number of one hashtag for each user#\n",
    "result_hashtag_df = grouped_df.groupBy(\"username\").agg(\n",
    "    log(2.0, F.max(\"count\")).alias(\"hashtags\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "hashtag_mean = result_hashtag_df.agg({\"hashtags\": \"mean\"}).collect()[0][0]\n",
    "hashtag_stddev = result_hashtag_df.agg({\"hashtags\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "result_hashtag_df = result_hashtag_df.withColumn(\"hashtags_zscore\", (col(\"hashtags\") - hashtag_mean) / hashtag_stddev)\n",
    "result_hashtag_df = result_hashtag_df.withColumn(\"hashtags_normalized\", ((least(col(\"hashtags_zscore\"), constant_column) + 3) / 6)*100)\n",
    "result_hashtag_df = result_hashtag_df.drop(\"hashtags_zscore\", \"hashtags\")\n",
    "#Display preview\n",
    "# result_hashtag_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba53e30-52b4-4ef0-b25d-f323bfd87f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_non_rt = avg_tweet_volume.join(avg_tweet_sentimentality, on=\"username\", how=\"inner\")\n",
    "# df_final = df_final_non_rt.join(df_normalized_follow, on=\"username\", how=\"left\").join(df_quote_percentage, on=\"username\", how=\"inner\").join(df_account_age, on=\"username\", how=\"inner\").join(username_counts, on=\"username\", how=\"inner\")\n",
    "df_final = avg_tweet_sentimentality_and_volume.join(df_normalized_follow, on=\"username\", how=\"left\").join(result_hashtag_df, on=\"username\", how=\"inner\")\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0056683-3a06-4569-bc9b-f34ba994de87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n0   [79.4582301   9.37295388 31.12492265 50.31629143 47.50125698 50.02783938\n 44.16757506  0.45206337 44.22859348]\n1   [  9.75376011 101.75124524  31.28379159  53.80605094  51.90431381\n  51.38576525  44.45289773   4.09480885  44.99443181]\n2   [22.67255065 26.58061289 57.09833993 73.8459879  48.0474814  48.50011235\n 81.62421914  8.22598116 81.26155144]\n3   [16.48347323 35.95984086 33.90163911 52.8513481  55.27651539 53.29812745\n 48.82990557 92.09263685 48.26641704]\n4   [ 29.85606252  25.38780931  37.39058825  64.68661733 200.\n  39.19916836  53.35428681   2.64267172  53.58557445]\n5   [19.17825515 48.33130063 32.4104372  46.16476758 48.42624243 48.06500434\n 46.58622082  0.79323226 46.63354119]\n6   [13.71288403  5.38467493 31.19247773 50.24374736 50.13561785 50.34012812\n 44.44514756  0.56561086 44.70452818]\n7   [86.06376644 10.26917972 31.34332314 54.08307502 57.38662859 54.89789563\n 44.77784645 94.97874396 44.7525081 ]\nPredictions:\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "selected_columns = [col for col in df_final.columns if col not in ['username', 'userid']]\n",
    "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_final)\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 8\n",
    "\n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans().setK(k).setSeed(1)\n",
    "\n",
    "# Fit the model to the assembled DataFrame\n",
    "model = kmeans.fit(df_assembled)\n",
    "\n",
    "# Get the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "predictions = model.transform(df_assembled)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cluster Centers:\")\n",
    "for index, center in enumerate(centers):\n",
    "    print(index, \" \", center)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "# predictions.select(\"username\", \"features\", \"prediction\").display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1544050290315258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SENG 550 Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
