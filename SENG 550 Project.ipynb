{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c5ab32-cbbd-4007-a11a-a9b826a9ae8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read all csv files into a single RDD\n",
    "import os.path\n",
    "\n",
    "# reads all filenames that match the patter. Currently only two within the DBFS, 0819 and 0820\n",
    "files = '/FileStore/tables/*_UkraineCombinedTweetsDeduped.csv'\n",
    "tweets = spark.read.csv(files,header=True,sep=\",\",multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f66fdf-f3d6-48de-8aba-f52bff4340cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[55]: 'Dear vaccine advocate\\n\\nDo take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\\n\\nRegards\\n#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu'"
     ]
    }
   ],
   "source": [
    "# remove columns not needed\n",
    "dropped_tweets = tweets.drop(\"original_tweet_id\", \\\n",
    "                            \"original_tweet_userid\", \\\n",
    "                            \"original_tweet_username\", \\\n",
    "                            \"in_reply_to_status_id\", \\\n",
    "                            \"in_reply_to_user_id\", \\\n",
    "                            \"in_reply_to_screen_name\", \\\n",
    "                            \"quoted_status_id\", \\\n",
    "                            \"quoted_status_userid\", \\\n",
    "                            \"quoted_status_username\", \\\n",
    "                            \"extractedts\")\n",
    "\n",
    "# dropped_tweets.show(1)\n",
    "\n",
    "dropped_tweets.first()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beac797f-ca81-4595-9479-e78524833a0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only english tweets\n",
    "english_tweets = dropped_tweets.filter(dropped_tweets.language == \"en\" )\n",
    "# english_tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206a62ff-9baa-4c98-97d6-774cbbc35cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only non-retweets\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "non_rt_tweets = english_tweets.filter(lower(english_tweets.is_retweet) == \"false\")\n",
    "# non_rt_tweets.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e3870e-2165-495d-8301-ab524d797f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/lib/python3.9/site-packages (0.17.1)\nRequirement already satisfied: nltk>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/lib/python3.9/site-packages (from textblob) (3.8.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.66.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2023.10.3)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.1)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d4f265-6cec-4fe6-bc01-82e8d11b9de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/lib/python3.9/site-packages (3.3.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-774815d8-bb32-4137-ba86-f8942231027b/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%sh pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52f56c5-b8b2-46c0-81d4-173d015275a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, least, lit, log\n",
    "\n",
    "def z_score_normalize(df, column, rangeVal = 100):\n",
    "    mean = df.agg({column: \"mean\"}).collect()[0][0]\n",
    "    stddev = df.agg({column: \"stddev\"}).collect()[0][0]\n",
    "    print(\"Mean \", mean, \" Stddev \", stddev)\n",
    "    df = df.withColumn(column + \"_zscore\", (col(column) - mean) / stddev)\n",
    "    constant_column = lit(9)\n",
    "    df = df.withColumn(column + \"_normalized\", ((least(col(column + \"_zscore\"), constant_column) + 3) / 6)*rangeVal)\n",
    "    return df.drop(column + \"_zscore\", column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb0ab8-53d7-41b3-a4f9-8b23f81a3318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  2.322582736921418  Stddev  1.2361977064188903\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf, to_date, least, lit, log\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# get average number of tweets per day from each user\n",
    "\n",
    "# make copy of dataframe and convert each tweet created time stamp into a time stamp object\n",
    "df = non_rt_tweets\n",
    "df = df.withColumn(\"date\", to_date(col(\"tweetcreatedts\")))\n",
    "\n",
    "# create window partitioned by user and ordered by date\n",
    "window = Window.partitionBy(\"username\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the number of tweets per day for each user\n",
    "df = df.withColumn(\"daily_tweet_count\", F.count(\"text\").over(window))\n",
    "\n",
    "# calculate average sentiment for each user\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_positive_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['pos']\n",
    "\n",
    "def get_negative_score(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['neg']\n",
    "\n",
    "# Register the sentiment analysis function as a Spark UDF\n",
    "sentiment_analysis_pos_udf = udf(get_positive_score, FloatType())\n",
    "sentiment_analysis_neg_udf = udf(get_negative_score, FloatType())\n",
    "\n",
    "# Apply sentiment analysis to each tweet and create a new column\n",
    "df = df.withColumn(\"positivity\", sentiment_analysis_pos_udf(col(\"text\")))\n",
    "df = df.withColumn(\"negativity\", sentiment_analysis_neg_udf(col(\"text\")))\n",
    "\n",
    "# Calculate the average sentimentality for each user and the average number of tweets per day\n",
    "avg_tweet_sentimentality_and_volume = df.groupBy(\"username\").agg(\n",
    "    # ((F.avg(\"sentiment_score\") + 1) * 50 * 2).alias(\"avg_sentimentality\"),\n",
    "    F.avg(\"daily_tweet_count\").alias(\"avg_tweets_per_day\"),\n",
    "    (F.avg(\"positivity\") * 300).alias(\"avg_positivity\"),\n",
    "    (F.avg(\"negativity\") * 300).alias(\"avg_negativity\")\n",
    ")\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = avg_tweet_sentimentality_and_volume.withColumn(\"avg_tweets_per_day\", log(1.5, (col(\"avg_tweets_per_day\") + 1)))\n",
    "\n",
    "avg_tweet_sentimentality_and_volume = z_score_normalize(avg_tweet_sentimentality_and_volume, \"avg_tweets_per_day\", 70)\n",
    "\n",
    "# avg_tweet_sentimentality_and_volume.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40af10c-e252-4192-a9da-f981613aca9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  2.5241207964271424  Stddev  1.0998834882567181\nMean  2.9733940303156325  Stddev  1.0303353712421732\nMean  229418239.08552757  Stddev  151826870.36694467\nMean  0.5971869335914884  Stddev  1.0701291491893592\n"
     ]
    }
   ],
   "source": [
    "# Normalize following and followers\n",
    "from pyspark.sql.functions import col, mean, count, sum, expr, least, lit, unix_timestamp, current_timestamp, log\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "current_time = current_timestamp()\n",
    "\n",
    "# Make sure you operate on unique users, in case of duplicates get the average for the user\n",
    "df_unique_users = english_tweets.groupBy(\"username\", \"usercreatedts\").agg(\n",
    "    # sum((col(\"is_quote_status\") == \"True\").cast(\"int\")).alias(\"is_quote_status_true_count\"),\n",
    "    # sum((col(\"is_quote_status\") == \"False\").cast(\"int\")).alias(\"is_quote_status_false_count\"),\n",
    "    mean(\"followers\").alias(\"followers\"),\n",
    "    mean(\"following\").alias(\"following\"),\n",
    "    count(\"username\").alias(\"tweet_count\"),\n",
    "    (unix_timestamp(current_time) - unix_timestamp(expr(\"substring(usercreatedts, 1, 19)\"))).cast(LongType()).alias(\"account_age\")\n",
    ")\n",
    "\n",
    "df_unique_users = df_unique_users.withColumn(\"followers\", log(10.0, col(\"followers\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"following\", log(7.0, col(\"following\")))\n",
    "df_unique_users = df_unique_users.withColumn(\"tweet_count\", log(2.0, col(\"tweet_count\")))\n",
    "\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"followers\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"following\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"account_age\", 100)\n",
    "df_unique_users = z_score_normalize(df_unique_users, \"tweet_count\", 100)\n",
    "\n",
    "# Unique users\n",
    "# df_unique_users = df_unique_users.withColumn(\"percentage_of_quotes\", 100*(col(\"is_quote_status_true_count\") / (col(\"is_quote_status_true_count\") + col(\"is_quote_status_false_count\"))))\n",
    "\n",
    "# df_normalized_follow = df_unique_users.drop(\"is_quote_status_true_count\", \"is_quote_status_false_count\", \"usercreatedts\")\n",
    "df_normalized_follow = df_unique_users.drop(\"usercreatedts\")\n",
    "\n",
    "# display(df_normalized_follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3e45cc-5dec-4a8b-95af-35b6afe8e513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  0.30934627195174147  Stddev  0.6061457140010845\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "#Returns Hashtags within a string as an array\n",
    "def extractHastags(tweet):\n",
    "    return re.findall(r\"#(\\w+)\",tweet)\n",
    "\n",
    "\n",
    "#Resgister extractHastags as a Spark UDF\n",
    "extract_hashtags_udf = F.udf(extractHastags, ArrayType(StringType()))\n",
    "\n",
    "#Drop unused columns\n",
    "hashtag_df = english_tweets.drop(\"acctdesc\", \\\n",
    "                                \"location\", \\\n",
    "                                \"following\", \\\n",
    "                                \"followers\", \\\n",
    "                                \"totaltweets\", \\\n",
    "                                \"usercreatedts\", \\\n",
    "                                \"tweetcreatedts\", \\\n",
    "                                \"retweetcount\", \\\n",
    "                                \"language\", \\\n",
    "                                \"coordinates\", \\\n",
    "                                \"favorite_count\", \\\n",
    "                                \"is_retweet\", \\\n",
    "                                \"is_quote_status\", \\\n",
    "                                \"_c0\", \\\n",
    "                                \"userid\", \\\n",
    "                                \"tweetid\", \\\n",
    "                                )\n",
    "\n",
    "#Map Hashtags to users\n",
    "hashtag_df = hashtag_df.withColumn(\"hashtags\", extract_hashtags_udf(col(\"text\")))\n",
    "\n",
    "#Explode hastag list\n",
    "hashtag_df = hashtag_df.select(\"username\", F.explode(\"hashtags\").alias(\"hashtag\"))\n",
    "\n",
    "#Count the number of each hashtag by user\n",
    "grouped_hashtag_df = hashtag_df.groupBy(\"username\", \"hashtag\").count()\n",
    "\n",
    "#Calculate the max number of one hashtag for each user#\n",
    "result_hashtag_df = grouped_hashtag_df.groupBy(\"username\").agg(\n",
    "    log(3.0, F.max(\"count\")).alias(\"hashtags\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "result_hashtag_df = z_score_normalize(result_hashtag_df, \"hashtags\", 100)\n",
    "\n",
    "#Display preview\n",
    "# result_hashtag_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5adc535f-279c-473f-8a85-014353cfd6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "#Drop unused columns\n",
    "count_df = english_tweets.drop(\"acctdesc\", \\\n",
    "                                \"location\", \\\n",
    "                                \"following\", \\\n",
    "                                \"followers\", \\\n",
    "                                \"text\", \\\n",
    "                                \"usercreatedts\", \\\n",
    "                                \"tweetcreatedts\", \\\n",
    "                                \"retweetcount\", \\\n",
    "                                \"language\", \\\n",
    "                                \"coordinates\", \\\n",
    "                                \"favorite_count\", \\\n",
    "                                \"is_retweet\", \\\n",
    "                                \"is_quote_status\", \\\n",
    "                                \"_c0\", \\\n",
    "                                \"userid\", \\\n",
    "                                \"tweetid\", \\\n",
    "                                \"hashtags\"\n",
    "                                )\n",
    "\n",
    "def calc_ratio(arr):\n",
    "    count = arr[0]\n",
    "    diff = arr[1]\n",
    "    if diff:\n",
    "        if count > diff:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return (count/diff)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "calc_ratio_udf = F.udf(calc_ratio, FloatType())\n",
    "\n",
    "count_df1 = count_df.groupBy(\"username\").count()\n",
    "\n",
    "#Calculate the max number of one hashtag for each user\n",
    "count_df2 = count_df.groupBy(\"username\").agg(\n",
    "    F.max(\"totaltweets\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "count_df3 = count_df.groupBy(\"username\").agg(\n",
    "    F.min(\"totaltweets\")\n",
    ").orderBy(\"username\")\n",
    "\n",
    "proportion_df = count_df1.join(count_df2,on=\"username\",how=\"left\")\n",
    "\n",
    "range_df = proportion_df.join(count_df3,on=\"username\",how=\"left\")\n",
    "\n",
    "\n",
    "range_df = range_df.withColumn(\"diff\", col(\"max(totaltweets)\")- col(\"min(totaltweets)\"))\n",
    "range_df = range_df.withColumn(\"ratio\", calc_ratio_udf(F.array( col(\"count\"),col(\"diff\") ))*100)\n",
    "range_df = range_df.drop(\"max(totaltweets)\", \"min(totaltweets)\", \"diff\", \"count\")\n",
    "\n",
    "# range_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba53e30-52b4-4ef0-b25d-f323bfd87f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_non_rt = avg_tweet_volume.join(avg_tweet_sentimentality, on=\"username\", how=\"inner\")\n",
    "# df_final = df_final_non_rt.join(df_normalized_follow, on=\"username\", how=\"left\").join(df_quote_percentage, on=\"username\", how=\"inner\").join(df_account_age, on=\"username\", how=\"inner\").join(username_counts, on=\"username\", how=\"inner\")\n",
    "df_final = avg_tweet_sentimentality_and_volume.join(df_normalized_follow, on=\"username\", how=\"left\").join(result_hashtag_df, on=\"username\", how=\"inner\").join(range_df, on=\"username\", how=\"inner\")\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0056683-3a06-4569-bc9b-f34ba994de87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n0   [  9.14200492 100.48599027  30.26834179  47.82672865  51.05152151\n  50.66494932  42.53453009  43.35722855  98.54069279]\n1   [82.7814457   9.29091813 30.53934796 48.96346286 49.44739353 50.97575095\n 42.95271519 43.63884781 99.16565903]\n2   [ 29.85606252  25.38780931  37.39058825  64.68661733 200.\n  39.19917363  53.35428681  53.58557445  89.19950815]\n3   [ 30.19044101  29.22029024  32.4981296  200.          20.06825146\n  32.16931753  45.92567963  47.32875744  96.67519283]\n4   [ 21.12671511  28.80272759  72.63779682  52.7548906   47.87848687\n  46.94364474 100.07984428 101.47399378  59.44494417]\n5   [27.97708792 29.02599296 43.84739212 54.39161563 53.19011047 51.58233122\n 66.86610903 62.10411814 16.86527215]\n6   [20.01454532 46.79519077 31.78881102 48.79459857 50.43089744 50.06545369\n 45.03967029 45.80146326 99.26816788]\n7   [13.31589541  4.42427897 30.90755156 50.35551916 50.55965457 50.62290304\n 43.5600609  44.51392678 99.62917139]\nPredictions:\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "selected_columns = [col for col in df_final.columns if col not in ['username', 'userid']]\n",
    "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_final)\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 8\n",
    "\n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans().setK(k).setSeed(1)\n",
    "\n",
    "# Fit the model to the assembled DataFrame\n",
    "model = kmeans.fit(df_assembled)\n",
    "\n",
    "# Get the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "predictions = model.transform(df_assembled)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cluster Centers:\")\n",
    "for index, center in enumerate(centers):\n",
    "    print(index, \" \", center)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "# predictions.select(\"username\", \"features\", \"prediction\").display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2054386082191072,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SENG 550 Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
